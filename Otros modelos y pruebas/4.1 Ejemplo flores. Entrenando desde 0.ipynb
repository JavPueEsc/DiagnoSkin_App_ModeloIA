{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flowers.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Descargamos el data set donde estarán las imágenes de las flores. Para ello he tenido\n",
    "que instalar por consola el módulo wget. Al igual que tensorflow_datasets, sirve\n",
    "para descargar datasets pero es menos especifico para el entrenamiento de ia.\n",
    "'''\n",
    "\n",
    "import wget \n",
    "\n",
    "wget.download('https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/flowers.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Extraemos el dataset del archivo .zip en el que viene.\n",
    "'''\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('flowers.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "podemos ver que tenemos 5 clases de flores diferentes, distribuidas en 5 carpetas diferentes. \n",
    "Cada carpeta contiene varios ejemplos de flores de la categoría en cuestión.\n",
    "'''\n",
    "import os \n",
    "\n",
    "PATH = 'flowers'\n",
    "\n",
    "classes = os.listdir(PATH)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoría: daisy. Imágenes: 769\n",
      "Categoría: dandelion. Imágenes: 1055\n",
      "Categoría: rose. Imágenes: 784\n",
      "Categoría: sunflower. Imágenes: 734\n",
      "Categoría: tulip. Imágenes: 984\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Vemos los archivos que hay en cada carpeta.\n",
    "'''\n",
    "imgs, labels = [], []\n",
    "\n",
    "for i, lab in enumerate(classes):\n",
    "  paths = os.listdir(f'{PATH}/{lab}')\n",
    "  print(f'Categoría: {lab}. Imágenes: {len(paths)}')\n",
    "  paths = [p for p in paths if p[-3:] == \"jpg\"]\n",
    "  imgs += [f'{PATH}/{lab}/{img}' for img in paths]\n",
    "  labels += [i]*len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Opción 1 . Ver imágenes de una carpeta del dataset\n",
    "'''\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Ruta a la carpeta que contiene las imágenes\n",
    "image_folder = 'flowers/daisy'\n",
    "\n",
    "# Listar todos los archivos en la carpeta\n",
    "images = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "\n",
    "# Número de imágenes a mostrar\n",
    "num_images = len(images)\n",
    "\n",
    "# Configurar la figura\n",
    "plt.figure(figsize=(15, 15))  # Ajusta el tamaño de la ventana\n",
    "\n",
    "# Mostrar cada imagen\n",
    "for i, img_name in enumerate(images):\n",
    "    img_path = os.path.join(image_folder, img_name)\n",
    "    img = mpimg.imread(img_path)\n",
    "\n",
    "    plt.subplot(160, 5, i+1)  # Puedes ajustar los números (5, 5) según el número de imágenes\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Desactivar los ejes\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Opción 2. Ver imágenes de una carpeta del dataset <-- ESTA SE VE MEJOR\n",
    "'''\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Ruta a la carpeta que contiene las imágenes\n",
    "image_folder = 'flowers/daisy'\n",
    "\n",
    "# Listar todos los archivos en la carpeta\n",
    "images = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "\n",
    "# Mostrar cada imagen\n",
    "for img_name in images:\n",
    "    img_path = os.path.join(image_folder, img_name)\n",
    "    display(Image(filename=img_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Opción 3. Ver imágeens aleatorias del dataset junto a aetiquetas <-- ES LA QUE USA EL CHICO\n",
    "DEL VIDEO QUE ESTOY SIGUIENDO.\n",
    "'''\n",
    "import random \n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3,5, figsize=(10,6))\n",
    "for _ax in axs:\n",
    "  for ax in _ax:\n",
    "    ix = random.randint(0, len(imgs)-1)\n",
    "    img = io.imread(imgs[ix])\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(classes[labels[ix]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3458, 865)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Dividimos el dataset; cogemos unas imagenes para entrenar y otras para validar.\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_imgs, test_imgs, train_labels, test_labels = train_test_split(imgs, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "len(train_imgs), len(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Creamos nuestros objetos Dataset y DataLoader para poder darle las imágenes a nuestros modelos.\n",
    "Estas imágenes deben estar normalizadas entre 0 y 1. tenemos que cambiar als dimensiones\n",
    "eso, lo explica el video de Sensio en el min 2:40 en adelante.\n",
    "'''\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, X, y, trans, device):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.trans = trans\n",
    "    self.device = device\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, ix):\n",
    "    # cargar la imágen\n",
    "    img = io.imread(self.X[ix])\n",
    "    # aplicar transformaciones\n",
    "    if self.trans:\n",
    "      img = self.trans(image=img)[\"image\"]\n",
    "    return torch.from_numpy(img / 255.).float().permute(2,0,1), torch.tensor(self.y[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3458, 865)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' En esta parte hacemos un redimensionado de las imagenes para que estén todas al mismo tamaño.\n",
    "'''\n",
    "import albumentations as A\n",
    "\n",
    "trans = A.Compose([\n",
    "    A.Resize(224, 224)\n",
    "])\n",
    "\n",
    "dataset = {\n",
    "    'train': Dataset(train_imgs, train_labels, trans, device), \n",
    "    'test': Dataset(test_imgs, test_labels, trans, device)\n",
    "}\n",
    "\n",
    "len(dataset['train']), len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True, pin_memory=True), \n",
    "    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False)\n",
    "}\n",
    "\n",
    "imgs, labels = next(iter(dataloader['train']))\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Vamos a escoger la arquitectura resnet para hacer nuestro clasificador. \n",
    "De este modelo usarmos todas las capas excepto la última, \n",
    "la cual sustituiremos por una nueva capa lineal para llevar a cabo la clasificación en 5 clases.\n",
    "\n",
    "Descargamos la restnet 18 porque tiene menos capas y es más fácil de entrenar.\n",
    "\n",
    "Imprimimos toda la lista de las capas que tiene\n",
    "Exolica las capas en Min.4.22\n",
    "\n",
    "De ste modelo usaremos todas las capas menos la última capa que la vamos a sustituir\n",
    "(consiste en la última linea de la salida de este codigo). nuestra ultima capa detbe tener\n",
    "5 clases y no 512 como la tiene actualmente. Gracias a esto usamos un modelo que sabe \"Ver\"\n",
    "pero modificamos la ultima parte que es la que vamos a entrenar para que diferencie entre las 5 \n",
    "clases de flores\n",
    "'''\n",
    "import torchvision\n",
    "\n",
    "resnet = torchvision.models.resnet18()\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer entrenamiento; Pretrained = false, freeze = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Definimos un nuevo modelo con las modificaciones que comentamos anteriormente \n",
    "(cambiar la ultima capa). Descargamos de nuevo resnet.\n",
    "La propiedad Pretrained hace que se asignen los pesos y sesgos ya entrenados. \n",
    "La propiedd Freeze da la posibilidad de congelar la red para que una vez asignados los pesos\n",
    "no se entrenen más y se queden como están.\n",
    "para congelar la red\n",
    "'''\n",
    "class Model(torch.nn.Module):\n",
    "  def __init__(self, n_outputs=5, pretrained=False, freeze=False):\n",
    "    super().__init__()\n",
    "    # descargamos resnet\n",
    "    resnet = torchvision.models.resnet18(pretrained=pretrained)\n",
    "    # nos quedamos con todas las capas menos la última\n",
    "    self.resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "    if freeze:\n",
    "      for param in self.resnet.parameters():\n",
    "        param.requires_grad=False\n",
    "    # añadimos una nueva capa lineal para llevar a cabo la clasificación\n",
    "    #Esta capa es la única que entrenaremos si ponememos freeze = false.\n",
    "    self.fc = torch.nn.Linear(512, 5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.resnet(x)\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    x = self.fc(x)\n",
    "    return x\n",
    "\n",
    "  def unfreeze(self):\n",
    "    for param in self.resnet.parameters():\n",
    "        param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dekad\\Desktop\\GrupoStudium\\3. Segundo DAM\\7. Proyecto integrado\\Espacio de trabajo\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dekad\\Desktop\\GrupoStudium\\3. Segundo DAM\\7. Proyecto integrado\\Espacio de trabajo\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Instanciamos nuestro modelo\n",
    "'''\n",
    "model = Model()\n",
    "outputs = model(torch.randn(64, 3, 224, 224))\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Función de entrenamiento estándar\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def fit(model, dataloader, epochs=5, lr=1e-2):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss, train_acc = [], []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            X, y = batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
    "            train_acc.append(acc)\n",
    "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
    "        bar = tqdm(dataloader['test'])\n",
    "        val_loss, val_acc = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in bar:\n",
    "                X, y = batch\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_hat = model(X)\n",
    "                loss = criterion(y_hat, y)\n",
    "                val_loss.append(loss.item())\n",
    "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
    "                val_acc.append(acc)\n",
    "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
    "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando desde cero\n",
    "En primer lugar vamos a entrenar nuestro modelo desde cero para ver qué métricas podemos obtener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dekad\\Desktop\\GrupoStudium\\3. Segundo DAM\\7. Proyecto integrado\\Espacio de trabajo\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dekad\\Desktop\\GrupoStudium\\3. Segundo DAM\\7. Proyecto integrado\\Espacio de trabajo\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "loss 1.35410 acc 0.42756: 100%|██████████| 55/55 [09:13<00:00, 10.07s/it]\n",
      "val_loss 9.04716 val_acc 0.22440: 100%|██████████| 4/4 [01:01<00:00, 15.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 loss 1.35410 val_loss 9.04716 acc 0.42756 val_acc 0.22440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 1.16039 acc 0.53153: 100%|██████████| 55/55 [08:17<00:00,  9.05s/it]\n",
      "val_loss 3.55095 val_acc 0.37270: 100%|██████████| 4/4 [00:50<00:00, 12.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 loss 1.16039 val_loss 3.55095 acc 0.53153 val_acc 0.37270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 1.05617 acc 0.58324: 100%|██████████| 55/55 [08:20<00:00,  9.09s/it]\n",
      "val_loss 1.91332 val_acc 0.30385: 100%|██████████| 4/4 [00:48<00:00, 12.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 loss 1.05617 val_loss 1.91332 acc 0.58324 val_acc 0.30385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.98443 acc 0.63295: 100%|██████████| 55/55 [08:06<00:00,  8.85s/it]\n",
      "val_loss 3.64727 val_acc 0.31552: 100%|██████████| 4/4 [00:27<00:00,  6.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 loss 0.98443 val_loss 3.64727 acc 0.63295 val_acc 0.31552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.96362 acc 0.62557: 100%|██████████| 55/55 [05:30<00:00,  6.01s/it]\n",
      "val_loss 1.79447 val_acc 0.35724: 100%|██████████| 4/4 [00:33<00:00,  8.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 loss 0.96362 val_loss 1.79447 acc 0.62557 val_acc 0.35724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.88225 acc 0.66761: 100%|██████████| 55/55 [05:52<00:00,  6.40s/it]\n",
      "val_loss 1.34856 val_acc 0.51898: 100%|██████████| 4/4 [00:36<00:00,  9.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 loss 0.88225 val_loss 1.34856 acc 0.66761 val_acc 0.51898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.81063 acc 0.68295: 100%|██████████| 55/55 [06:03<00:00,  6.62s/it]\n",
      "val_loss 3.26115 val_acc 0.37208: 100%|██████████| 4/4 [00:35<00:00,  8.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 loss 0.81063 val_loss 3.26115 acc 0.68295 val_acc 0.37208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.79758 acc 0.69233: 100%|██████████| 55/55 [06:02<00:00,  6.59s/it]\n",
      "val_loss 6.34670 val_acc 0.24420: 100%|██████████| 4/4 [00:35<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 loss 0.79758 val_loss 6.34670 acc 0.69233 val_acc 0.24420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.73839 acc 0.72955: 100%|██████████| 55/55 [06:43<00:00,  7.34s/it]\n",
      "val_loss 4.97404 val_acc 0.32349: 100%|██████████| 4/4 [00:43<00:00, 10.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 loss 0.73839 val_loss 4.97404 acc 0.72955 val_acc 0.32349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.69354 acc 0.74545: 100%|██████████| 55/55 [06:23<00:00,  6.97s/it]\n",
      "val_loss 4.15637 val_acc 0.30716: 100%|██████████| 4/4 [00:38<00:00,  9.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 loss 0.69354 val_loss 4.15637 acc 0.74545 val_acc 0.30716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.70890 acc 0.73778: 100%|██████████| 55/55 [06:27<00:00,  7.04s/it]\n",
      "val_loss 5.58630 val_acc 0.25131: 100%|██████████| 4/4 [00:37<00:00,  9.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 loss 0.70890 val_loss 5.58630 acc 0.73778 val_acc 0.25131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.67041 acc 0.75000: 100%|██████████| 55/55 [06:18<00:00,  6.87s/it]\n",
      "val_loss 11.42029 val_acc 0.25557: 100%|██████████| 4/4 [00:37<00:00,  9.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 loss 0.67041 val_loss 11.42029 acc 0.75000 val_acc 0.25557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.63299 acc 0.76847: 100%|██████████| 55/55 [06:28<00:00,  7.07s/it]\n",
      "val_loss 1.30408 val_acc 0.52457: 100%|██████████| 4/4 [00:34<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 loss 0.63299 val_loss 1.30408 acc 0.76847 val_acc 0.52457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.55976 acc 0.79744: 100%|██████████| 55/55 [06:23<00:00,  6.98s/it]\n",
      "val_loss 1.83809 val_acc 0.52730: 100%|██████████| 4/4 [00:36<00:00,  9.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 loss 0.55976 val_loss 1.83809 acc 0.79744 val_acc 0.52730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.58296 acc 0.78977: 100%|██████████| 55/55 [06:37<00:00,  7.23s/it]\n",
      "val_loss 6.17793 val_acc 0.36243: 100%|██████████| 4/4 [00:38<00:00,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 loss 0.58296 val_loss 6.17793 acc 0.78977 val_acc 0.36243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "fit(model, dataloader, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
